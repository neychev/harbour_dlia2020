{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: the provided initial code is an adaptation of the [Starter code for Stanford CS224n default final project on SQuAD 2.0](https://github.com/chrischute/squad) which is shared under MIT License. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does initial preprocessing for the SberQuAD dataset and will give you the starting point in this assignment. If it looks too complex and/or time/resourse-expensive, you may stick to homework05 as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing\n",
    "This code is a bit changed version of the code from `setup.py`. If you want to work with the SQuAD dataset, stick to the original instructions from the https://github.com/chrischute/squad repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab, uncomment the following lines \n",
    "\n",
    "# !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/homeworks/homework04/args.py -nc\n",
    "# !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/homeworks/homework04/layers.py -nc\n",
    "# !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/homeworks/homework04/models.py -nc\n",
    "# !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/homeworks/homework04/setup.py -nc\n",
    "# !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/homeworks/homework04/test.py -nc\n",
    "# !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/homeworks/homework04/train.py -nc\n",
    "# !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/homeworks/homework04/util.py -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab, uncomment the following lines \n",
    "\n",
    "# !pip install ujson\n",
    "# !pip install tensorboardX\n",
    "# !pip install pymorphy2==0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train a model on SQuAD.\n",
    "\n",
    "Author:\n",
    "    Chris Chute (chute@stanford.edu)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "import util\n",
    "\n",
    "from args import get_train_args\n",
    "from collections import OrderedDict\n",
    "from json import dumps\n",
    "from models import BiDAF\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from ujson import load as json_load\n",
    "from util import collate_fn, SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"./data\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./save\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the SberQuAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://files.deeppavlov.ai/datasets/sber_squad_clean-v1.1.tar.gz -nc -O ./data/sber_squad_clean-v1.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xzvf ./data/sber_squad_clean-v1.1.tar.gz\n",
    "! mv train-v1.1.json data\n",
    "! mv dev-v1.1.json data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the word vectors (this may take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec -nc -O ./data/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the preprocessing for the SberQuAD dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/train-v1.1.json'\n",
    "dev_file = './data/dev-v1.1.json'\n",
    "glove_file = './data/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if needed\n",
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell may take a while (usually 10 minutes or less)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training set and use it to decide on the word/character vocabularies\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "train_examples, train_eval = process_file(train_file, \"train\", word_counter, char_counter, nlp)\n",
    "word_emb_mat, word2idx_dict = get_embedding(\n",
    "    word_counter, 'word', emb_file=glove_file, vec_size=300, num_vectors=1560132)\n",
    "char_emb_mat, char2idx_dict = get_embedding(\n",
    "    char_counter, 'char', emb_file=None, vec_size=64)\n",
    "\n",
    "\n",
    "dev_examples, dev_eval = process_file(dev_file, \"dev\", word_counter, char_counter, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record_file = './data/train.npz'\n",
    "dev_record_file = './data/dev.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import add_common_args, get_setup_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreiving the default arguments for the preprocessing script\n",
    "_args = get_setup_args(bypass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_features(_args, train_examples, \"train\", train_record_file, word2idx_dict, char2idx_dict)\n",
    "dev_meta = build_features(_args, dev_examples, \"dev\", dev_record_file, word2idx_dict, char2idx_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(_args.word_emb_file, word_emb_mat, message=\"word embedding\")\n",
    "save(_args.char_emb_file, char_emb_mat, message=\"char embedding\")\n",
    "save(_args.train_eval_file, train_eval, message=\"train eval\")\n",
    "save(_args.dev_eval_file, dev_eval, message=\"dev eval\")\n",
    "save(_args.word2idx_file, word2idx_dict, message=\"word dictionary\")\n",
    "save(_args.char2idx_file, char2idx_dict, message=\"char dictionary\")\n",
    "save(_args.dev_meta_file, dev_meta, message=\"dev meta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The experiment\n",
    "\n",
    "Now you are almost ready to go. You may follow these steps to begin (or just start your experiments here).\n",
    "\n",
    "1. Try running the `train.py` script from the console (or via `!`) (default command-line arguments are ok for the start). If will run the BiDAF model on the preprocessed data. Set `--use_squad_v2` flag to False (SberQuAD is similar to SQuAD v1.1).\n",
    "\n",
    "Example code (be careful with the path and the names of the variables):\n",
    "```\n",
    "python train.py --name first_run_on_sberquad --use_squad_v2 False\n",
    "```\n",
    "\n",
    "2. After if finishes (might take an 1-2-3 hours depending on the hardware), evaluate your model on the `dev` set and measure the quality.\n",
    "Example code (be careful with the path and the names of the variables):\n",
    "```\n",
    " python test.py --split dev --load_path ./save/train/first_run_on_sberquad-02/best.pth.tar --name best_evaluation_experiment\n",
    "```\n",
    "The result should be similar to the following:\n",
    "```\n",
    ">>> Dev NLL: 02.47, F1: 75.62, EM: 55.73, AvNA: 99.42\n",
    "```\n",
    "\n",
    "The [DeepPavlov's RuBERT](http://docs.deeppavlov.ai/en/master/features/models/squad.html) achieves $F1 = 84.60\\pm0.11$ and $EM = 66.30\\pm0.24$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here comes your quest: try to improve the quality of this QA system. \n",
    "\n",
    "This is a very creative assignment. It is all about experimenting, trying different approaches (and a lot of computations). But if you wish to stick to some numbers, try to increase F1 at least by $5$ points.\n",
    "\n",
    "Here are some ideas that might help you on your way:\n",
    "* Try adapting the optimization hyperparameters/network structure to Russian language (the baseline is designed for English SQuAD dataset).\n",
    "* Incorporating the additional information about the data (like PoS tags) might be a good idea.\n",
    "* __Distilling the knowledge from a pre-trained RuBERT__ (e.g. try to use the predictions of the model we've discussed on `week10` as soft targets).\n",
    "* Or anything else.\n",
    "\n",
    "\n",
    "And, first of all, read the initial code carefully.\n",
    "\n",
    "\n",
    "Good luck! Feel free to share your results :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
